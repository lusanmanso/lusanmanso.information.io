{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrS_tU9R6ztW"
      },
      "source": [
        "# APARTADO 2: WEB SCRAPING TRADICIONAL Y SCRAPING DE INTERFACES CON BEAUTIFULSOUP Y SELENIUM\n",
        "## 2.1 Introducción\n",
        "En caso de no disponer de una API que nos permita acceder a datos textuales en un sitio web, o si la información que nos ofrece la plataforma no se encuentra expuesta de forma estructurada, tenemos la opción de usar web scraping. Es una técnica que consiste en obtener información de forma directa de las páginas, gracias a la interpretación de la estructura y el contenido del documento HTML. Una vez descargado o renderizado el HTML, podemos apuntar a secciones o nodos específicos para extraer texto, enlaces u otros atributos.\n",
        "En el contexto de la ingeniería de software, la habilidad de raspar sitios resulta valiosa para recopilar corpus de texto, datos de reseñas, foros de discusión, listados de productos, entre otros. El scraping puede presentarse en dos grandes escenarios:\n",
        "- Sitios estáticos, en los que el HTML contiene la mayoría o la totalidad de la información deseada en el momento de la respuesta HTTP inicial.\n",
        "- Sitios dinámicos, sustentados por JavaScript, lo que implica que parte del contenido se genere tras la carga de la página, o requiera la simulación de acciones del usuario (clics, scroll, inicios de sesión).\n",
        "Para el primer caso, BeautifulSoup suele ser suficiente; para el segundo, suele necesitarse un navegador automatizado como Selenium. Existen situaciones híbridas donde, antes de decidir utilizar Selenium, conviene analizar las llamadas de red a través de BurpSuite o las Developer Tools del navegador, a fin de discernir si el contenido se podría obtener con requests directamente desde un endpoint interno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84S4uog06ztZ"
      },
      "source": [
        "## 2.2 Estructura de los sitios web (HTML, CSS, JavaScript)\n",
        "### 2.2.1 Fundamentos de la estructura HTML\n",
        "Un sitio web se describe con HTML (HyperText Markup Language). Este lenguaje define los nodos que componen la página, usualmente organizados de manera jerárquica (árbol DOM). Cada nodo se compone de:\n",
        "1. Etiqueta de inicio (p. ej., `<div class=\"post\">`), que incluye el nombre del elemento y atributos.\n",
        "2. Contenido interno: puede ser texto crudo, otros nodos anidados (hijos) o ambos.\n",
        "3. Etiqueta de cierre (p. ej., `</div>`), excepto en elementos vacíos (e.g. `<img />`).\n",
        "\n",
        "La organización de HTML se clasifica a menudo en varios tipos de etiquetas, que cada desarrollador o framework utiliza para fines semánticos o de maquetación:\n",
        "\n",
        "- Estructura y disposición general: páginas modernas suelen utilizar `<header>`, `<nav>`, `<main>`, `<section>`, `<article>`, `<aside>` y `<footer>` para señalar la intención de cada bloque. Por ejemplo, un `<header>` se encuentra al inicio de la página, un `<footer>` al final, `<nav>` para menús, `<article>` para contenido principal.\n",
        "- Agrupación de contenido: etiquetas como `<div>` y `<span>` son elementos genéricos de bloque o en línea, respectivamente, para agrupar contenido. `<p>` indica párrafos. Muchas webs reusan `<div>` para todo tipo de “cajas” y `<span>` para “fragmentos en línea.”\n",
        "- Semántica y resaltado: etiquetas como `<h1>`, `<h2>`, `<h3>` indican encabezados en distintos niveles. `<b>`, `<strong>` y `<i>` realzan palabras, aunque `<strong>` e `<em>` tienen un significado semántico distinto al meramente visual de `<b>` o `<i>`.\n",
        "- Inserción de contenido externo: se hallan elementos como `<audio>`, `<video>`, `<iframe>` y `<img>`. De hecho, un `<iframe>` puede embeber otra página entera. Estos pueden presentar retos para scraping si el contenido real se aloja en un documento adicional o en un streaming.\n",
        "- Tablas: se definen con `<table>`, `<tr>` (fila), `<td>` (celda). En sitios antiguos, las tablas se usaban para maquetar la página entera; en sitios modernos, se reservan para datos tabulares genuinos. Por ejemplo, una tabla de precios de productos o un ranking de usuarios.\n",
        "- Formularios: `<form>` abarca todo el bloque de campos, `<input>` define un campo (texto, checkbox, etc.), `<select>` para menús desplegables, `<button>` o `<input type=\"submit\">` para enviar. Para scraping, es clave entender cómo enviar datos al servidor (p. ej. un login) o cómo se estructuran los parámetros que se envían (e.g. `name=\"username\"`).\n",
        "\n",
        "https://itwebtutorials.mga.edu/html/chp2/document-structure.aspx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT3vS4dP6ztZ"
      },
      "source": [
        "\n",
        "En el siguiente código de HTML se ilustra una estructura sencilla:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>Web U-Tad</title>\n",
        "</head>\n",
        "<body>\n",
        "  <header>\n",
        "    <h1>Bienvenido a la U-tad!</h1>\n",
        "    <nav>\n",
        "      <ul>\n",
        "        <li><a href=\"/index.html\">Inicio</a></li>\n",
        "        <li><a href=\"/about.html\">Sobre nosotros</a></li>\n",
        "      </ul>\n",
        "    </nav>\n",
        "  </header>\n",
        "  <main>\n",
        "    <section class=\"blog-entry\">\n",
        "      <h2>Primer post</h2>\n",
        "      <p>Estudiar en la U-Tad es genial!</p>\n",
        "      <span class=\"author\">Creado por Jesús</span>\n",
        "      <aside>Más información</aside>\n",
        "    </section>\n",
        "    <section class=\"blog-entry\">\n",
        "      <h2>Segundo post</h2>\n",
        "      <p>Estoy mejorando mucho mi conocimiento en Python!</p>\n",
        "      <span class=\"author\">Creado por Jesús</span>\n",
        "    </section>\n",
        "  </main>\n",
        "  <footer>\n",
        "    <p>&copy; 2025 U-Tad.</p>\n",
        "  </footer>\n",
        "</body>\n",
        "</html>\n",
        "```"
      ],
      "metadata": {
        "id": "F7OBKS0v68hV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKouniJE6ztb"
      },
      "source": [
        "Este ejemplo refleja cómo se usan `<header>` y `<footer>` para la cabecera y pie de página. El `<nav>` aloja enlaces de navegación, `<main>` contiene el contenido principal, dividido en `<section>` con clase blog-entry y, dentro, `<h2>` para el título y `<p>` para el texto. Nodos como `<aside>` añaden información extra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmdjyT7K6ztb"
      },
      "source": [
        "### 2.2.2 El rol de CSS y JavaScript\n",
        "El CSS (Cascading Style Sheets) define la apariencia de los nodos (colores, márgenes, tipografía). Para el scraping, lo importante es poder localizar elementos a través de selectores. Por ejemplo, un bloque con clase blog-entry se selecciona con .blog-entry en CSS. Si deseamos un `<span>` de autor con clase author, en CSS se define .author. Al usar librerías como BeautifulSoup o Selenium, esos selectores simplifican la extracción: `soup.select(\".blog-entry .author\")`\n",
        "CSS se caracteriza por ser un lenguaje simple, permitir el anidamiento de instrucciones, ser estándar en todos los navegadores y permitir personalizar la apariencia de las páginas. Además, se puede insertar dentro de una etiqueta HTML o en un fichero aparte.\n",
        "La estructura de CSS pues se compone de selectores, reglas y valores como:\n",
        "```css\n",
        "/* Selector: p (selecciona todos los párrafos) */\n",
        "p {\n",
        "  /* Regla: color con el valor azul */\n",
        "  color: blue;\n",
        "}\n",
        "```\n",
        "JavaScript, se trata de un lenguaje de programación interpretado que nos permite interactuar con los elementos de la web y \"darles vida\". Gracias a esto, la web, internet y las redes sociales se han desarrollado hasta el nivel en el que se encuentran hoy en día.\n",
        "Los sitios dinámicos usan JavaScript para alterar el DOM (Document Object Model, interfaz que facilita la representación, manteniendo una estructura definida, del documento) en tiempo de ejecución, insertar datos o responder a eventos. Este factor es crucial para el scraping: un `requests.get(url)` puede devolver un HTML vacío si la mayor parte del contenido se inyecta tras la carga con JavaScript. En tal caso, se requiere Selenium (o alguna técnica que reproduzca llamadas AJAX) para obtener el estado final del DOM.\n",
        "JavaScript se caracteriza por ser un lenguaje de alto nivel, orientado a objetos y estar débilmente tipado. Además, es un lenguaje Interpretado que interactúa con el DOM y se ejecuta en cliente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP3z4KoZ6ztb"
      },
      "source": [
        "## 2.3 Herramientas de inspección (Dev Tools y BurpSuite)\n",
        "Las Developer Tools (Chrome/Firefox) son el método más directo para explorar la estructura y el tráfico de una página. Nos ayudan a:\n",
        "- Ver el DOM real en la pestaña “Elements” (o “Inspector” en Firefox).\n",
        "- Seleccionar elementos y ver sus atributos, clases y estilos.\n",
        "- Revisar peticiones HTTP en “Network,” confirmando si hay endpoints JSON, tokens, o secuencias de requests para la paginación.\n",
        "\n",
        "Esta exploración inicial permite definir la estrategia de scraping: “¿está todo en un `<div class=\"post\">`, o necesito descubrir requests AJAX con JSON?”\n",
        "\n",
        "BurpSuite es una suite más avanzada orientada a pruebas de seguridad, pero muy útil para scraping. Configurando el navegador para pasar por el proxy de BurpSuite, podemos interceptar cada request y respuesta. Esto revela:\n",
        "\n",
        "- Tokens en cabeceras o formularios.\n",
        "- Endpoints internos que devuelven datos en JSON.\n",
        "- Redirecciones a otros dominios.\n",
        "\n",
        "Una vez hallamos esos endpoints internos, a menudo podemos replicar la petición con requests sin necesidad de Selenium ni parseo HTML. Por ejemplo, si la aplicación Angular llama a `GET /api/posts?page=2`, podríamos interceptar esa request en Burp, ver qué parámetros y cookies requiere, y replicar en Python. De esta forma, podemos ser capaces de autenticarnos o hacer otras acciones incluso aunque las APIs no estén pensadas o preparadas para ello mediante el uso o modificación de cookies, headers, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpd0edrW6ztc"
      },
      "source": [
        "## 2.4 Query selectors\n",
        "Para scrapear de forma efectiva, necesitamos seleccionar los nodos de interés dentro del DOM de la página. Los query selectors son la vía principal para decirle a nuestras herramientas (un navegador, BeautifulSoup o Selenium) qué elementos queremos. Estos selectores se basan en la misma sintaxis que utiliza CSS, combinando etiquetas, clases, ids, atributos y pseudoelementos para localizar con precisión cualquier nodo del árbol HTML.\n",
        "Un query selector es una cadena que describe uno o varios elementos del DOM conforme a reglas definidas en el estándar CSS. Los selectores pueden combinarse jerárquicamente (`div > p`) o unirse por lógica (`h2.title, p.description`). El navegador, BeautifulSoup y Selenium ofrecen métodos para usarlos:\n",
        "- En el navegador, en la consola de DevTools, podemos emplear `document.querySelector(\"...\")` (devuelve el primer elemento) o `document.querySelectorAll(\"...\")` (devuelve todos los que coincidan).\n",
        "- En BeautifulSoup, disponemos de `soup.select(\"...\")` para obtener una lista de elementos, y `select_one(\"...\")` para un único elemento.\n",
        "- En Selenium, se puede emplear `driver.find_element(\"css selector\", \"...\")` o `driver.find_elements(\"css selector\", \"...\")`\n",
        "\n",
        "La sintaxis de los selectores es muy flexible. Entre los selectores más comunes encontramos:\n",
        "\n",
        "| Tipo de Selector | Símbolo / Ejemplo | Función | Ejemplo de uso |\n",
        "|------------------|-------------------|---------|----------------|\n",
        "| Por etiqueta (tag) | `\"p\", \"div\", \"span\"` | Selecciona todos los elementos de un tipo concreto de etiqueta. | CSS: `p { color: blue; }` <br> JS: `document.querySelectorAll(\"p\")` <br> BeautifulSoup: `soup.select(\"p\")` |\n",
        "| Por id | `#header, #main, #footer` | Selecciona un elemento cuyo atributo id coincida con el especificado. (El id debería ser único en la página). | CSS: `#header { background: black; }` <br> JS: `document.querySelector(\"#header\")` <br> BeautifulSoup: `soup.select(\"#header\")` |\n",
        "| Por clase | `.blog-entry, .author` | Selecciona todos los elementos que posean la clase dada. Una etiqueta puede tener varias clases: `<div class=\"blog-entry post\">`. | CSS: `.blog-entry { margin-bottom: 20px; }` <br> JS: `document.querySelectorAll(\".blog-entry\")` <br> BeautifulSoup: `soup.select(\".blog-entry\")` |\n",
        "| Combinado (clases) | `.card.highlight` | Selecciona elementos con ambas clases. `<div class=\"card highlight\">`. | CSS: `.card.highlight { border: 1px solid; }` <br> JS: `document.querySelectorAll(\".card.highlight\")` <br> BS: `soup.select(\".card.highlight\")` |\n",
        "| Jerárquico (descend.) | `\"div p\"` | Selecciona `<p>` que estén dentro de un `<div>` en cualquier nivel de anidación. | CSS: `div p { color: red; }` <br> JS: `document.querySelectorAll(\"div p\")` <br> BS: `soup.select(\"div p\")` |\n",
        "| Jerárquico (hijo) | `\"div > p\"` | Selecciona `<p>` que sean hijos directos de `<div>`. | CSS: `div > p { font-size: 14px; }` <br> JS: `document.querySelectorAll(\"div > p\")` <br> BS: `soup.select(\"div > p\")` |\n",
        "| Hermano adyacente | `\"div + p\"` | Selecciona `<p>` que aparezca justo después de un `<div>` (en el mismo nivel). | CSS: `div + p { margin-top: 5px; }` <br> JS: `document.querySelectorAll(\"div + p\")` <br> BS: `soup.select(\"div + p\")` |\n",
        "| Hermano general | `\"div ~ p\"` | Selecciona todos los `<p>` posteriores al `<div>` dentro del mismo padre. | CSS: `div ~ p { color: green; }` <br> JS: `document.querySelectorAll(\"div ~ p\")` <br> BS: `soup.select(\"div ~ p\")` |\n",
        "| Selector de atributo | `img[src], a[href=\"...\"]` | Selecciona elementos que posean un atributo en particular. Se puede especificar el valor, p. ej. `a[href=\"login.html\"]`. | CSS: `img[src] { border: 1px solid #ccc; }` <br> JS: `document.querySelectorAll('img[src]')` <br> BS: `soup.select('img[src]')` <br> Combinado: `soup.select('a[href=\"login.html\"]')` |\n",
        "| Múltiple | `\"h2, p.desc, div.content\"` | Selecciona `h2` o `p.desc` o `div.content`, es decir, la unión de tres criterios distintos. | CSS: `h2, p.desc, div.content { margin:10px; }` <br> JS: `document.querySelectorAll(\"h2, p.desc, div.content\")` <br> BS: `soup.select(\"h2, p.desc, div.content\")` |\n",
        "\n",
        "Supongamos que queremos `<a>` con clase \"button\" y cuyo atributo `href` contenga la palabra \"product\" pero que, además, sea un hijo directo de un `<li>` que esté dentro de un `<ul>` de clase \"nav-list\". El selector podría ser:\n",
        "\n",
        "`ul.nav-list > li > a.button[href*=\"product\"]`\n",
        "\n",
        "Una manera sencilla de validar los query selectors es desde la consola de desarrolladores del navegador. Para ello deberemos:\n",
        "\n",
        "1. Para este ejemplo iremos a la web: https://parascrapear.com/\n",
        "2. Abrir las herramientas de desarrollador y buscar la consola con “F12”, “Ctrl+Shift+I”, dependerá de nuestro navegador.\n",
        "3. En la consola debemos escribir `document.querySelectorAll(\"<TU_QUERY_SELECTOR>\")`. Antes de ejecutar el comando ya obtenemos una pequeña vista previa de cuanto vamos a obtener\n",
        "\n",
        "\n",
        "\n",
        "4. Obtengamos diferentes secciones de la página:\n",
        "\n",
        "```javascript\n",
        "// Citas\n",
        "document.querySelectorAll(\"blockquote p > q\")\n",
        "```\n",
        "\n",
        "\n",
        "```javascript\n",
        "// Categorias\n",
        "document.querySelectorAll(\"blockquote > p a.cat\")\n",
        "// Autores\n",
        "document.querySelectorAll(\"blockquote > footer a.author\")\n",
        "// Siguiente página\n",
        "document.querySelectorAll(\"a.next\")\n",
        "```\n",
        "\n",
        "De esta forma, podemos acceder a los nodos para posteriormente extraer sus datos. Si quisiésemos acceder al texto, deberíamos leer el elemento y la propiedad correspondiente. Por ejemplo:\n",
        "```javascript\n",
        "// Texto de siguiente página\n",
        "document.querySelectorAll(\"a.next\")[0].text\n",
        "// Output\n",
        "'Siguiente →'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvhqkBDn6ztd"
      },
      "source": [
        "## 2.5 Scraping tradicional (HTML estático) con BeautifulSoup\n",
        "BeautifulSoup es una librería Python para parsear HTML y XML de forma sencilla. Se encarga de convertir el HTML en un objeto DOM que podemos recorrer con métodos de búsqueda y selectores. Dado que en muchos sitios el contenido estático se encuentra en un HTML final, no necesitamos ejecutar JavaScript ni un navegador completo.\n",
        "El proceso típico sería:\n",
        "1. Descargamos la página con `requests.get(url).text`.\n",
        "2. Creamos un `BeautifulSoup(html, \\\"html.parser\\\")`.\n",
        "3. Buscamos nodos con `.find()`, `.find_all()`, `.select()`, extrayendo texto o atributos.\n",
        "\n",
        "Esta técnica es simple, rápida y eficiente, siempre que la página no requiera JS o interacción dinámica para mostrar el contenido. Gracias a los Query Selectors podremos identificar los nodos deseados y extraer su información.\n",
        "\n",
        "En cuanto al uso, podemos extraer información de atributos mediante `element[\\\"href\\\"]` o `element.get(\\\"href\\\")`. También podemos tratar el texto gracias a `.get_text(strip=True)` quita espacios extra. Para navegar, tenemos las opciones de `.parent`, `.children`, `.next_sibling`, y `.previous_sibling` se recorre el árbol, si la estructura es compleja.\n",
        "\n",
        "Para instalar la librería, simplemente deberemos ejecutar:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBTezwS86ztd"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSl38T-w6ztd"
      },
      "source": [
        "Si utilizamos el mismo ejemplo hecho con los Query Selectors, podemos obtener todo el contenido de los nodos identificados de la siguiente manera:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiewOMBA6zte"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.parascrapear.com\"\n",
        "res = requests.get(url)\n",
        "html = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "citas = html.select(\"blockquote p > q\")\n",
        "citas = [x.text for x in citas]\n",
        "print(f\"Citas: {citas}\")\n",
        "\n",
        "categorias = html.select(\"blockquote > p a.cat\")\n",
        "categorias = [x.text for x in categorias]\n",
        "print(f\"Categorias: {categorias}\")\n",
        "\n",
        "autores = html.select(\"blockquote > footer a.author\")\n",
        "autores = [x.text for x in autores]\n",
        "print(f\"Autores: {autores}\")\n",
        "\n",
        "next_page_data = html.select(\"a.next\")\n",
        "next_page = f'{url}{[x[\"href\"] for x in next_page_data][0]}'\n",
        "print(f\"Pagina siguiente: {next_page}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIlXqBWr6zte"
      },
      "source": [
        "Como podemos ver en el caso anterior, una vez extraídos los datos, observamos que aún sigue habiendo una “Página siguiente” que probablemente tenga más información. Una manera muy común para scrapear, sería hacer un pipeline que vaya extrayendo información hasta que no haya más páginas. Lo podemos ver en el siguiente ejemplo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VypneclF6zte"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_page(url):\n",
        "    res = requests.get(url)\n",
        "    html = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "    blockquotes = html.select(\"blockquote\")\n",
        "    quotes = []\n",
        "    cat = []\n",
        "    authors = []\n",
        "    for bq in blockquotes:\n",
        "        citas = bq.select_one(\"p > q\")\n",
        "        quotes.append(citas.text)\n",
        "\n",
        "        categorias = bq.select_one(\"p a.cat\")\n",
        "        cat.append(categorias.text)\n",
        "\n",
        "        autores = bq.select_one(\"footer a.author\")\n",
        "        authors.append(autores.text)\n",
        "\n",
        "    next_node = html.select(\"a.next\")\n",
        "\n",
        "    if len(next_node) > 0:\n",
        "        next_page = f'{url}{next_node[0][\"href\"]}'\n",
        "    else:\n",
        "        next_page = None\n",
        "\n",
        "    return quotes, cat, authors, next_page\n",
        "\n",
        "url = \"https://www.parascrapear.com/\"\n",
        "\n",
        "total_quotes = []\n",
        "total_cats = []\n",
        "total_authors = []\n",
        "while url is not None:\n",
        "    quotes, cat, authors, url = scrape_page(url)\n",
        "    total_authors += authors\n",
        "    total_quotes += quotes\n",
        "    total_cats += cat\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"quotes\": total_quotes,\n",
        "    \"cats\": total_cats,\n",
        "    \"authors\": total_authors\n",
        "})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD7AzhOJ6zte"
      },
      "source": [
        "## 2.6 Scraping dinámico con Selenium\n",
        "Selenium es un marco de automatización de navegadores que permite simular la interacción de un usuario real. Esto resulta necesario cuando las páginas dependen de JavaScript, realizan peticiones AJAX después de la carga inicial, o exigen iniciar sesión para mostrar contenido. Selenium abre un navegador (Chrome, Firefox, Edge) o su versión headless, y ejecuta el mismo código JS que el usuario vería en pantalla.\n",
        "\n",
        "La razón de usar Selenium en lugar de requests + BeautifulSoup es que:\n",
        "\n",
        "- Algunos sitios no cargan datos en el HTML inicial, sino que se los traen con JavaScript.\n",
        "- Hay sitios que solicitan logins, captchas o clicks en botones de “Load More.”\n",
        "- Se requieren acciones complejas, como seleccionar opciones en menús y esperar un response asíncrono.\n",
        "\n",
        "Selenium provee métodos para encontrar elementos (driver.find_element(...)) y leer propiedades como .text. Tras la ejecución de scripts, la página deviene en un estado final que podemos examinar.\n",
        "\n",
        "Además, Selenium dispone de dos modos principales:\n",
        "\n",
        "- Headless=True, con este modo toda la interacción con el navegador se hace “por detrás” de tal modo que en pantalla no se aprecia nada.\n",
        "- Headless=False, con este modo se abre una pestaña del navegador y se puede ver como Selenium interactúa con los componentes de la página web.\n",
        "\n",
        "Para instalar selenium simplemente deberemos ejecutar:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar Google Colab para arrancar el código de Selenium\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "metadata": {
        "id": "Ne8Vi2te_0DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ynL8_L76zte"
      },
      "source": [
        "Además, fuera de colab, será necesario tener instalado el ChromeDriver siguiendo: https://developer.chrome.com/docs/chromedriver/get-started\n",
        "\n",
        "La configuración e inicialización de Selenium se puede resumir en:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "# Inicializamos el ChromeDriver (o el driver que necesitemos según el navegador)\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') #No arrancamos el browser\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.headless = True\n",
        "wd = webdriver.Chrome(options=chrome_options) #'chromedriver',\n",
        "# Obtenemos la web deseada\n",
        "wd.get(\"https://www.google.com\")\n",
        "# Cerramos el driver para liberar los recursos utilizados\n",
        "wd.quit()"
      ],
      "metadata": {
        "id": "f2dEErdF83iO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uTKVKPa6ztf"
      },
      "source": [
        "En cuanto a la navegación y localización de elementos, Selenium ofrece numerosas opciones. De entre ellas, las básicas son:\n",
        "\n",
        "- driver.get(url): abre la página especificada.\n",
        "- driver.back(): navega a la página anterior en el historial.\n",
        "- driver.forward(): avanza a la siguiente página en el historial.\n",
        "- driver.refresh(): recarga la página actual.\n",
        "- Para la localización de elementos:\n",
        "  - find_element(By.ID, \"id\")\n",
        "  - find_element(By.NAME, \"name\")\n",
        "  - find_element(By.XPATH, \"xpath\")\n",
        "  - find_element(By.LINK_TEXT, \"link text\")\n",
        "  - find_element(By.PARTIAL_LINK_TEXT, \"partial link text\")\n",
        "  - find_element(By.TAG_NAME, \"tag name\")\n",
        "  - find_element(By.CLASS_NAME, \"class name\")\n",
        "  - find_element(By.CSS_SELECTOR, \"css selector\"), este sería el equivalente a los Query Selectors\n",
        "  \n",
        "¿Cómo se resolvería el caso de scrapeme.live con selenium? Veamos el siguiente ejemplo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XOJ0lp9v6ztf"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "\n",
        "# Configuramos el WebDriver\n",
        "def create_driver():\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless') #No arrancamos el browser\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.headless = True\n",
        "    wd = webdriver.Chrome(options=chrome_options)\n",
        "    #driver = webdriver.Chrome(options=chrome_options)\n",
        "    return wd\n",
        "\n",
        "url = \"https://scrapeme.live/shop/Bulbasaur/\"\n",
        "\n",
        "driver = create_driver()\n",
        "driver.get(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "driver.title # If it gives you \"Privacy Error\", then run the previous cell again."
      ],
      "metadata": {
        "id": "D7RICmOWAtht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pokemon_name = driver.find_element(by=By.CSS_SELECTOR, value=\".summary .product_title\").text\n",
        "print(f\"Pokemon Name: {pokemon_name}\")\n",
        "\n",
        "pokemon_price = driver.find_element(by=By.CSS_SELECTOR, value=\"p.price\").text\n",
        "print(f\"Pokemon Price: {pokemon_price}\")\n",
        "\n",
        "# Debemos cerrar el driver, de lo contrario no liberaremos los recursos del PC\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "oxRbFaX8_K-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DQswW0Q6ztf"
      },
      "source": [
        "Del mismo modo, si queremos definir un pipeline en el que scrapear todas las páginas para obtener la información de todos los elementos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi6yzGWk6ztf"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "\n",
        "# Configuramos el WebDriver\n",
        "def create_driver():\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless') #No arrancamos el browser\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.headless = True\n",
        "    wd = webdriver.Chrome(options=chrome_options)\n",
        "    #driver = webdriver.Chrome(options=chrome_options)\n",
        "    return wd\n",
        "\n",
        "# Función para obtener la información de un Pokémon\n",
        "def get_pokemon_info(url):\n",
        "    driver = create_driver()\n",
        "    driver.get(url)\n",
        "\n",
        "    try:\n",
        "        pokemon_name_qs = \".summary .product_title\"\n",
        "        pokemon_price_qs = \"p.price\"\n",
        "        pokemon_desc_qs = \".summary .woocommerce-product-details__short-description p\"\n",
        "        pokemon_stock_qs = \".summary .stock\"\n",
        "        pokemon_image_qs = \".woocommerce-product-gallery__image .wp-post-image\"\n",
        "        pokemon_price_curr_qs = \".woocommerce-Price-currencySymbol\"\n",
        "\n",
        "        pokemon_name = driver.find_element(By.CSS_SELECTOR, pokemon_name_qs).text\n",
        "        print(f\"Scraping {pokemon_name}\")\n",
        "\n",
        "        pokemon_price_curr = driver.find_element(By.CSS_SELECTOR, pokemon_price_curr_qs).text\n",
        "        pokemon_price = driver.find_element(By.CSS_SELECTOR, pokemon_price_qs).text\n",
        "        pokemon_price = float(pokemon_price.replace(pokemon_price_curr, \"\").strip())\n",
        "\n",
        "        pokemon_desc = driver.find_element(By.CSS_SELECTOR, pokemon_desc_qs).text\n",
        "        pokemon_stock = int(driver.find_element(By.CSS_SELECTOR, pokemon_stock_qs).text.split(\" \")[0])\n",
        "        pokemon_image = driver.find_element(By.CSS_SELECTOR, pokemon_image_qs).get_attribute(\"src\")\n",
        "\n",
        "    except Exception as e:\n",
        "        driver.quit()\n",
        "        return f\"Error with page: {url}, {e}\"\n",
        "\n",
        "    driver.quit()\n",
        "    return {\"name\": pokemon_name,\n",
        "            \"image_url\": pokemon_image,\n",
        "            \"description\": pokemon_desc,\n",
        "            \"price\": pokemon_price,\n",
        "            \"currency\": pokemon_price_curr,\n",
        "            \"stock\": pokemon_stock}\n",
        "\n",
        "# Función para obtener los elementos de la página\n",
        "def get_page_elements(url):\n",
        "    driver = create_driver()\n",
        "    correct= False\n",
        "    #while()\n",
        "    driver.get(url)\n",
        "    print(driver.title)\n",
        "\n",
        "    try:\n",
        "        print(f\"Scraping: {url}\")\n",
        "\n",
        "        urls_qs = \"ul .product .woocommerce-LoopProduct-link\"\n",
        "        next_url_qs = \".page-numbers .next\"\n",
        "\n",
        "        urls = [elem.get_attribute(\"href\") for elem in driver.find_elements(By.CSS_SELECTOR, urls_qs)]\n",
        "\n",
        "        next_url_elem = driver.find_elements(By.CSS_SELECTOR, next_url_qs)\n",
        "        next_url = next_url_elem[0].get_attribute(\"href\") if next_url_elem else None\n",
        "\n",
        "    except Exception as e:\n",
        "        driver.quit()\n",
        "        return f\"Error with: {url}, {e}\"\n",
        "\n",
        "    driver.quit()\n",
        "    return urls, next_url\n",
        "\n",
        "# Scrapeamos todas las páginas\n",
        "next_page = \"https://scrapeme.live/shop/\"\n",
        "pokemons_list = []\n",
        "\n",
        "\n",
        "while next_page is not None:\n",
        "    try:\n",
        "      urls, next_page = get_page_elements(next_page)\n",
        "      for url in urls:\n",
        "          pokemon_info = get_pokemon_info(url)\n",
        "          pokemons_list.append(pokemon_info)\n",
        "      break # Solo leemos la primera página\n",
        "    except Exception as e:\n",
        "      #print(\"Error\")\n",
        "      print(f\"Error scraping page: {e}\")\n",
        "      print(\"----------\")\n",
        "\n",
        "df = pd.DataFrame(pokemons_list)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La principal diferencia que podemos observar es a nivel de rendimiento ya que Selenium va a tardar muchos minutos más que BeatifulSoup. Para el caso de los Pokemon en Scrapeme.live estamos hablando de que **puede tardar del orden de horas** en recorrer las casi 50 páginas."
      ],
      "metadata": {
        "id": "NsRbgsSZLPSw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlhQefRZ6ztf"
      },
      "source": [
        "\n",
        "## 2.7 Selenium VS BeautifulSoup\n",
        "Elegir la herramienta adecuada es fundamental para garantizar la eficiencia y efectividad del proceso de extracción de datos. BeautifulSoup y Selenium son dos de las bibliotecas más populares utilizadas para el parsing de contenido web, cada una con sus propias ventajas y limitaciones. Mientras que BeautifulSoup se destaca por su simplicidad y velocidad en el manejo de páginas estáticas, Selenium ofrece una capacidad superior para interactuar con sitios dinámicos que dependen de JavaScript y requieren acciones de usuario más complejas. Para facilitar la toma de decisiones informadas, analizaremos la siguiente tabla:\n",
        "\n",
        "| Aspecto                | BeautifulSoup                                      | Selenium                                      |\n",
        "|------------------------|----------------------------------------------------|-----------------------------------------------|\n",
        "| Necesidad de JS        | Para webs estáticas sin necesidad de JS            | Para webs dinámicas con necesidad de JS       |\n",
        "| Velocidad              | Muy rápida                                         | Lenta                                         |\n",
        "| Interactividad         | No puede interactuar                               | Sí puede interactuar con botones y otros      |\n",
        "| Necesidad de recursos  | Muy baja                                           | Muy alta                                      |\n",
        "| Casos de uso           | Sencillos, para páginas con HTML                   | Complejos para webs modernas                  |\n",
        "\n",
        "\n",
        "## 2.8 Extra\n",
        "En la práctica, hay escenarios complejos donde la página expone parte de la información en JavaScript, y además requiere un token de sesión que se obtiene al iniciar la página. Para analizar el funcionamiento y saber qué solución debemos implementar, podremos realizar el siguiente proceso:\n",
        "1. Configurar el navegador para redirigir el tráfico por el proxy de BurpSuite.\n",
        "2. Navegar manualmente por la web e interceptar las requests. Si se detecta un GET /api/data?page=2 que devuelve un JSON con el contenido en lugar de HTML, podemos replicarlo en un script con requests.\n",
        "3. Extraer el token “X-CSRF-Token” o “Authorization Bearer” desde el interceptado.\n",
        "4. Probar con un script Python que incluya las cabeceras y parámetros descubiertos.\n",
        "5. Descartar Selenium si no es preciso, o mantenerlo si la web requiere iniciar sesión mediante un formulario JS, en cuyo caso Selenium reproduciría la acción de login, y el proxy de BurpSuite confirmaría la existencia de cookies o tokens.\n",
        "De esta manera, BurpSuite funciona como herramienta forense para descubrir qué pasa en el “backstage” de la aplicación web. Después, se decide si bastan requests y BeautifulSoup para parsear la respuesta HTML/JSON, o si se precisa Selenium para secuencias más intrincadas.\n",
        "\n",
        "## 2.9 Ética y legalidad\n",
        "La extracción masiva de datos puede topar con límites técnicos y legales. Ciertos sitios prohíben explicitamente el scraping o su reutilización en sus términos de servicio. Además, existe el archivo robots.txt, un mecanismo por el que se sugiere qué rutas el scraper no debe acceder. Aunque no es legalmente vinculante en todos los casos, se considera buena práctica respetarlo.\n",
        "Saturar un sitio con demasiadas requests en poco tiempo puede considerarse un ataque (como un DDoS) o perjudicar su rendimiento. Se recomienda introducir pausas y no descargar miles de páginas simultáneamente. Por último, se recogen datos que identifiquen personas, se deben conocer las regulaciones (como GDPR en Europa) para no incurrir en violaciones de privacidad.\n",
        "\n",
        "## 2.10 Enlaces de interés\n",
        "• BurpSuite community edition: https://portswigger.net/burp/communitydownload\n",
        "\n",
        "• BeautifulSoup documentation: https://beautiful-soup-4.readthedocs.io/\n",
        "\n",
        "• Selenium documentation: https://www.selenium.dev/documentation/\n",
        "\n",
        "• Ética en scraping: https://www.meritdata-tech.com/resources/blog/data/web-scraping-best-practices-ethical-data-collection/\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}